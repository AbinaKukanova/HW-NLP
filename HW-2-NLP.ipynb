{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №2\n",
    "\n",
    "В этой домашке вам будет нужно найти тексты на русском языке (размер корпуса не менее 200 слов), в которых будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную – с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!\n",
    "\n",
    "(1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем нужные библиотеки\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В корпус я включила слова с неизвестными словарю корнем, но образованные с помощью продуктивных аффиксов, авторские \"придуманные слова\", сложные слова, у которых вторая часть совпадает со словами в словаре Зализняка, слова с неизвестными корнями без продуктивных суффиксов, редкие и нестандартные формы, которые признаются ненормативными или окказиональными в языке и аббвериатуры. Для разметки я использовала граммемы, которые используются в pymorphy2 для русского языка, в pymorphy2 используются словари OpenCoprora и граммемы, принятые в OpenCorpora с небольшими изменениями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "korpus = pd.read_csv('final_korpus.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_taggers = []\n",
    "for pos_tager in korpus['POS']:\n",
    "    pos_taggers.append(pos_tager)\n",
    "\n",
    "pos_taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Второе и Третье задание \n",
    "Потом вам будет нужно взять три POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них.Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy pos tagger\n",
    "spacy_taggers = []\n",
    "for line in korpus['Word']:\n",
    "    spacy_taggers.append(nlp(line)[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(spacy_taggers):\n",
    "    if n == 'ADJ':\n",
    "        spacy_taggers[i] = 'ADJF'\n",
    "    if n == 'ADV':\n",
    "        spacy_taggers[i] = 'ADVB'\n",
    "    if n == 'PROPN':\n",
    "        spacy_taggers[i] = 'NOUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30288461538461536\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.metrics\n",
    "\n",
    "r = sklearn.metrics.confusion_matrix(pos_taggers, spacy_taggers)\n",
    "r = numpy.flip(r)\n",
    "\n",
    "acc = (r[0][0] + r[-1][-1]) / numpy.sum(r)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'analysis': [{'lex': 'ультраженственный',\n",
       "     'wt': 0.4541687794,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'A=(пр,ед,полн,жен|дат,ед,полн,жен|род,ед,полн,жен|твор,ед,полн,жен)'}],\n",
       "   'text': 'ультраженственной'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'миллионометр',\n",
       "     'wt': 1,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,муж,неод=род,ед'}],\n",
       "   'text': 'миллионометра'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'гильоше',\n",
       "     'wt': 0.2741620489,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,сред,неод=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "   'text': 'гильоше'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'полуколебание',\n",
       "     'wt': 0.5196742212,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,сред,неод=род,мн'}],\n",
       "   'text': 'полуколебаний'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'баухаус',\n",
       "     'wt': 1,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,муж,неод=(вин,ед|им,ед)'}],\n",
       "   'text': 'баухаус'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'скелетонный',\n",
       "     'wt': 0.8459597832,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}],\n",
       "   'text': 'скелетонных'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'турбийон',\n",
       "     'wt': 0.8054043934,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,муж,неод=род,ед'}],\n",
       "   'text': 'турбийона'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'напылить',\n",
       "     'wt': 1,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'V,сов,пе=прош,мн,прич,кр,страд'}],\n",
       "   'text': 'напылены'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'ультралегкий',\n",
       "     'wt': 1,\n",
       "     'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}],\n",
       "   'text': 'ультралегким'},\n",
       "  {'text': '\\n'}],\n",
       " [{'analysis': [{'lex': 'алмазоподобный',\n",
       "     'wt': 1,\n",
       "     'gr': 'A,полн=(вин,ед,муж,неод|им,ед,муж)'}],\n",
       "   'text': 'алмазоподобный'},\n",
       "  {'text': '\\n'}]]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mystem pos tagger \n",
    "mystem_korpus = []\n",
    "for line in korpus['Word']:\n",
    "    ana = m.analyze(line)\n",
    "    mystem_korpus.append(ana)\n",
    "\n",
    "mystem_korpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_taggers = []\n",
    "for word in mystem_korpus:\n",
    "    if word[0]['analysis'] == []:\n",
    "        mystem_taggers.append('None')\n",
    "    else:\n",
    "        gr = word[0]['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        mystem_taggers.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(mystem_taggers):\n",
    "    if n == 'A':\n",
    "        mystem_taggers[i] = 'ADJF'\n",
    "    if n == 'ADV':\n",
    "        mystem_taggers[i] = 'ADVB'\n",
    "    if n == 'S':\n",
    "        mystem_taggers[i] = 'NOUN'\n",
    "    if n == 'V':\n",
    "        mystem_taggers[i] = 'VERB'\n",
    "    if n == 'PART':\n",
    "        mystem_taggers[i] = 'PRCL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33653846153846156\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.metrics\n",
    "\n",
    "r = sklearn.metrics.confusion_matrix(pos_taggers, mystem_taggers)\n",
    "r = numpy.flip(r)\n",
    "\n",
    "acc = (r[0][0] + r[-1][-1]) / numpy.sum(r)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymorphy pos tagger \n",
    "pymorphy_taggers = []\n",
    "for word in korpus['Word']:\n",
    "    if morph.parse(word)[0].tag.POS == None:\n",
    "        pymorphy_taggers.append('None')\n",
    "    else:\n",
    "        pymorphy_taggers.append(morph.parse(word)[0].tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1971153846153846\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.metrics\n",
    "\n",
    "r = sklearn.metrics.confusion_matrix(pos_taggers, pymorphy_taggers)\n",
    "r = numpy.flip(r)\n",
    "\n",
    "acc = (r[0][0] + r[-1][-1]) / numpy.sum(r)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы использовали теггеры pymorphy, pymystem и spacy. Из всех трех теггеров accuracy больше всех набрал теггер pymystem. Достоинства Mystem'a: хорошее качество разбора, по умолчанию разрешается частеречная омонимия (внутри части речи остается), при разборе учитывается контекст\n",
    "и совместим с разметкой НКРЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Четвертое задание \n",
    "\n",
    "Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за создание такого рода чанкера, балл за за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем датасет из прошлого домашнего задания, в той тетрадке я сохраняла этот файл и вызывается как selected, он уже есть на гитхабе\n",
    "selected = pd.read_csv('comments.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим на положительные и отрицательные\n",
    "positive_df = selected[selected['label_binary'] == 1.0]\n",
    "negative_df = selected[selected['label_binary'] == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция chunking\n",
    "Напишем функцию, которая будет выделять три типа словосочетаний, мы возьмем три шаблона: прилагательные и существительные, глагол и наречие, существительное и существительное. Прилагательное описывает существительное, признак предмета, в нем может содержаться положительная или негативная оценка, наречие тоже описывает признак действия, существительное тоже может иметь негативную или положительную оценку. Так как среди остальных теггеров лучше себя показал теггер pymystem, то используем его в этой функции и вытащим с помощью него части речи. Обязательно будем проверять, есть ли \"analysis\" в разборе, потому что может и не быть. Проверяем у каждого слова в предложении, есть ли его часть речи в нашем списке ent. Если нету, то добавляем в chunked_sent, а если есть, то смотрим составляет это слова одну из наших шаблонов. После этого добавляем в chunked_sent данные лексемы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Очень богатый мужчина не дал хорошо мало золотых монет туртсияхх ибиптахучить бедному мальчику'\n",
    "ent = (\"A\", \"V\", \"S\")\n",
    "def chunking(sent):\n",
    "    tagged = []\n",
    "    chunk = []\n",
    "    chunked_sent = []\n",
    "    ana = m.analyze(sent)\n",
    "    for word in ana:\n",
    "        if \"analysis\" in word:\n",
    "            if word['analysis'] == []:\n",
    "                chunked_sent.append(word['text'])\n",
    "            else:\n",
    "                tagged.append(word['analysis'][0])\n",
    "\n",
    "\n",
    "    for i, n in enumerate(tagged):\n",
    "        if i < (len(tagged) - 1):\n",
    "            if  tagged[i]['gr'].split('=')[0].split(',')[0] not in ent:\n",
    "                chunked_sent.append(tagged[i]['lex'])\n",
    "            else:\n",
    "                if tagged[i]['gr'].split('=')[0].split(',')[0] == \"A\":\n",
    "                    if tagged[i+1]['gr'].split('=')[0].split(',')[0] == \"S\":\n",
    "                        chunk.append(tagged[i]['lex'])\n",
    "                        chunk.append(tagged[i+1]['lex'])\n",
    "                        chunked_sent.append(chunk)\n",
    "                        chunk = []\n",
    "                elif tagged[i]['gr'].split('=')[0].split(',')[0] == \"V\":\n",
    "                    if tagged[i+1]['gr'].split('=')[0].split(',')[0] == \"ADV\":\n",
    "                        chunk.append(tagged[i]['lex'])\n",
    "                        chunk.append(tagged[i+1]['lex'])\n",
    "                        chunked_sent.append(chunk)\n",
    "                        chunk = []\n",
    "                elif tagged[i]['gr'].split('=')[0].split(',')[0] == \"S\":\n",
    "                    if tagged[i+1]['gr'].split('=')[0].split(',')[0] == \"S\":\n",
    "                        chunk.append(tagged[i]['lex'])\n",
    "                        chunk.append(tagged[i+1]['lex'])\n",
    "                        chunked_sent.append(chunk)\n",
    "                        chunk = []\n",
    "    return chunked_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#применим функцию chunking на нашем датасете\n",
    "positive_tokens = []\n",
    "for line in positive_df['lemm_clean_tex']:\n",
    "    positive_tokens.append(chunking(line))\n",
    "\n",
    "negative_tokens = []\n",
    "for line in negative_df['lemm_clean_tex']:\n",
    "    negative_tokens.append(chunking(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "positive_tokens = reduce(operator.concat, positive_tokens)\n",
    "negative_tokens = reduce(operator.concat, negative_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tuple = []\n",
    "for i in positive_tokens:\n",
    "    if type(i) is list:\n",
    "        positive_tuple.append(tuple(i))\n",
    "    else:\n",
    "        positive_tuple.append(i)\n",
    "\n",
    "negative_tuple = []\n",
    "for i in negative_tokens:\n",
    "    if type(i) is list:\n",
    "        negative_tuple.append(tuple(i))\n",
    "    else:\n",
    "        negative_tuple.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Возвращает словарь {слово: абсолютная частота встречаемости} на основе списка слов positive_tokens\n",
    "pos_freq = {}\n",
    "for pos_word in positive_tuple:\n",
    "    if pos_word in pos_freq:\n",
    "        pos_freq[pos_word] += 1\n",
    "    else:\n",
    "        pos_freq[pos_word] = 1\n",
    "\n",
    "neg_freq = {}\n",
    "for neg_word in negative_tuple:\n",
    "    if neg_word in neg_freq:\n",
    "        neg_freq[neg_word] += 1\n",
    "    else:\n",
    "        neg_freq[neg_word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выбросим слова, которые встречаются только 1-2 раза\n",
    "pos_words = []\n",
    "for pos_word, freq_word in pos_freq.items():\n",
    "    if freq_word > 2:\n",
    "        pos_words.append(pos_word)\n",
    "\n",
    "neg_words = []\n",
    "for neg_word, freq_word in neg_freq.items():\n",
    "    if freq_word > 2:\n",
    "        neg_words.append(neg_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = set(pos_words)\n",
    "neg_set = set(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pos = pos_set.difference(neg_set)\n",
    "diff_neg = neg_set.difference(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pos = pos_set.difference(neg_set)\n",
    "diff_neg = neg_set.difference(pos_set)\n",
    "print(diff_pos)\n",
    "print(diff_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем функцию из прошлого домашнего задания и немного ее переделаем под наше задание, используем в ней нашу функцию chunking и списки заменим на кортежи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(reply):\n",
    "    clear_text = re.sub(r'[^А-яЁё]+', ' ', text).lower()\n",
    "    lemmas = m.lemmatize(clear_text)\n",
    "    lemm_text = ''.join(lemmas)\n",
    "    tokens = word_tokenize(lemm_text)\n",
    "    joined_tokens= ' '.join(tokens)\n",
    "    chunked = chunking(joined_tokens)\n",
    "    sent_tuple = []\n",
    "    for i in chunked:\n",
    "        if type(i) is list:\n",
    "            sent_tuple.append(tuple(i))\n",
    "        else:\n",
    "            sent_tuple.append(i)\n",
    "\n",
    "    sample_set = set(tokens)\n",
    "    if sample_set.intersection(diff_pos) > sample_set.intersection(diff_neg):\n",
    "        print('positive')\n",
    "    else:\n",
    "        print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вытащим первые пять положительных и отрицательных комментариев, чтобы проверить функцию и рассчитать accuracy\n",
    "scrapped = {}\n",
    "for line in positive_df['text'].head():\n",
    "    scrapped[line] = \"positive\"\n",
    "\n",
    "for line in negative_df['text'].head():\n",
    "    scrapped[line] = \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(scrapped.values())\n",
    "scrapped_keys = list(scrapped.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "def sentiment_analysis(reply):\n",
    "    clear_text = re.sub(r'[^А-яЁё]+', ' ', text).lower()\n",
    "    lemmas = m.lemmatize(clear_text)\n",
    "    lemm_text = ''.join(lemmas)\n",
    "    tokens = word_tokenize(lemm_text)\n",
    "    joined_tokens= ' '.join(tokens)\n",
    "    chunked = chunking(joined_tokens)\n",
    "    sent_tuple = []\n",
    "    for i in chunked:\n",
    "        if type(i) is list:\n",
    "            sent_tuple.append(tuple(i))\n",
    "        else:\n",
    "            sent_tuple.append(i)\n",
    "\n",
    "    sample_set = set(tokens)\n",
    "    if sample_set.intersection(diff_pos) > sample_set.intersection(diff_neg):\n",
    "        b.append('positive')\n",
    "    else:\n",
    "        b.append('negative')\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scrapped_keys:\n",
    "    y_pred = sentiment_analysis(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "r = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "r = numpy.flip(r)\n",
    "\n",
    "acc = (r[0][0] + r[-1][-1]) / numpy.sum(r)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем, что accuracy равно 0.5 как и было в прошлом задании. Также можем взять NLTK VADER и с помощью него, разделить слова и словосочетания в нашем словаре на положительные, нейтральные и отрицательные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraPhrase = positive_tokens + negative_tokens\n",
    "extraPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['некоторые',\n",
       " ['список', 'любовь'],\n",
       " ['герой', 'р'],\n",
       " ['растрелять', 'правда'],\n",
       " 'правда',\n",
       " ['воевать', 'недавно'],\n",
       " 'недавно',\n",
       " ['неплохой', 'репортаж'],\n",
       " ['наградной', 'лист'],\n",
       " ['лист', 'бой']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five = extraPhrase[:10]\n",
    "first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    "def sentimentWordsFunct(x):\n",
    "    senti_list_temp = []    \n",
    "    for i in x:\n",
    "        if type(i) is list:\n",
    "            y = ' '.join(i)\n",
    "        else:\n",
    "            y = ''.join(i) \n",
    "\n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]    \n",
    "        sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "    \n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                if v < 0.0:\n",
    "                    sentiment_list.append((first, \"Negative\"))\n",
    "                elif v == 0.0:\n",
    "                    sentiment_list.append((first, \"Neutral\"))\n",
    "                else:\n",
    "                    sentiment_list.append((first, \"Positive\"))   \n",
    "    \n",
    "    return sentiment_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('некоторые', 'Neutral'),\n",
       " ('список любовь', 'Neutral'),\n",
       " ('герой р', 'Neutral'),\n",
       " ('растрелять правда', 'Neutral'),\n",
       " ('правда', 'Neutral'),\n",
       " ('воевать недавно', 'Neutral'),\n",
       " ('недавно', 'Neutral'),\n",
       " ('неплохой репортаж', 'Neutral'),\n",
       " ('наградной лист', 'Neutral'),\n",
       " ('лист бой', 'Neutral')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentWordsFunct(first_five)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
